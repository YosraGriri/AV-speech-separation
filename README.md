# Master's Thesis Project: Audio-Visual Speech Source Separation using Machine Learning and Beamforming

This README file provides an overview of the Audio-Visual Speech Source Separation for my master's thesis. It includes a brief description of the thesis,
its structure, and instructions for usage and setup.

## Table of Contents

- [Description](#description)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Installation](#installation)
- [Usage](#usage)
- [tba]

## Description

This project focuses on the development of an audio-visual speech source separation system that leverages machine learning and beamforming techniques. 
The goal is to separate and enhance the audio signals corresponding to different speakers in a multi-speaker scenario based on both audio and visual cues.

## Project Structure

The project is organized into the following directories and files:

- `data/`: Contains the datasets used for training and testing.
- `model/`: Includes machine learning model.
#- `notebooks/`: Jupyter notebooks for data exploration, model training, and evaluation.
- `src/`: Source code for the audio-visual speech source separation system.
- `README.md`: This README file.
- `LICENSE`: The project's license file.

Feel free to explore each directory for more details about their contents.

## Requirements

To run this project, you will need the following software and libraries:

- Python (>= 3.6)
- Jupyter Notebook (optional, for running notebooks)
- Additional Python packages (specified in `requirements.txt`)

Please refer to the installation section for setting up the required environment.

## Installation

1. Clone this repository to your local machine:

   ```bash
   git clone 

